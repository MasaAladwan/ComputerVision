{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The ReBlurSR dataset is designed for real-world blurred image super-resolution, containing high-resolution (HR, sharp) and low-resolution (LR, blurred) images with defocus and motion blur types. \n",
    "the file is 15.68 GB , with almost 3000 HR pictures \n",
    "\n",
    "what it contains : \n",
    "1. ALL_HR : High resolution (sharp) images \n",
    "2. ALL_mask : corresponding blur masks \n",
    "3. valid/defocus/LR/X4/, valid/motion/LR/X4/: Low-resolution (blurred) images\n",
    "4. .npy files : \n",
    "    - train_validation_class.npy: Labels images as train (0) or validation (1).\n",
    "    - defocus_motion_class.npy: Labels blur type as defocus (0) or motion (1).        \n",
    "\n",
    "Think of .npy files as label sheets for a box of photos:\n",
    "\n",
    "- Each photo (image in ALL_HR) has a sticky note with two labels:\n",
    "    - One says “train” or “validation” (train_validation_class.npy).\n",
    "    - Another says “defocus” or “motion” (defocus_motion_class.npy).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "defocus_lr_dir = '/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/valid/defocus/LR/X4'\n",
    "motion_lr_dir = '/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/valid/motion/LR/X4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_dir = '/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/all_HR'\n",
    "output_dir = \"/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/subset_dataset\"\n",
    "os.makedirs(os.path.join(output_dir,\"blurred\"),exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir,\"clear\"),exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "defocus_lr_images = [f for f in os.listdir(defocus_lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "motion_lr_images = [f for f in os.listdir(motion_lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "hr_images = [f for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_defocus = random.sample(defocus_lr_images, 250)\n",
    "selected_motion = random.sample(motion_lr_images, 250)\n",
    "selected_blurred = selected_defocus + selected_motion\n",
    "\n",
    "selected_clear = random.sample(hr_images, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 987.99it/s] \n",
      "100%|██████████| 500/500 [02:48<00:00,  2.97it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for img in tqdm(selected_blurred):\n",
    "    src = os.path.join(defocus_lr_dir if img in selected_defocus else motion_lr_dir, img)\n",
    "    shutil.copy(src, os.path.join(output_dir, \"blurred\", img))\n",
    "\n",
    "\n",
    "for img in tqdm(selected_clear):\n",
    "    src = os.path.join(hr_dir, img)\n",
    "    shutil.copy(src, os.path.join(output_dir, \"clear\", img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata = []\n",
    "for img in os.listdir(os.path.join(output_dir, \"blurred\")):\n",
    "    metadata.append({\"filename\": os.path.join(\"blurred\", img), \"label\": 1})\n",
    "for img in os.listdir(os.path.join(output_dir, \"clear\")):\n",
    "    metadata.append({\"filename\": os.path.join(\"clear\", img), \"label\": 0})\n",
    "df = pd.DataFrame(metadata)\n",
    "df.to_csv(os.path.join(output_dir, \"metadata.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'blurred/DIV2K_0853_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0082_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0082_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0061_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1479_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1485_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0037_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0288_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0805_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0224_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1483_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0224_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0067_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0019_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0019_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0006_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0842_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1416_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1429_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1429_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0033_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0829_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0104_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0063_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1473_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0035_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0191_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1429_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0006_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0058_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0224_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1483_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1483_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0862_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1477_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0280_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0280_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1466_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0037_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0069_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0008_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1407_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0035_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0199_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0035_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0191_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0191_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0054_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0065_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0024_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0284_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1414_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0206_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0809_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1480_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1435_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0279_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0279_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0055_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0268_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0294_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0893_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0023_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0023_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0870_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0055_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0055_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0240_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0240_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0068_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0130_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0048_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0057_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0159_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0223_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0296_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0223_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0296_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0136_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0287_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0880_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0863_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1437_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1437_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0055_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0055_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0240_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0240_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0144_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0089_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0839_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0062_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0279_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0279_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0882_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0114_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0114_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1480_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0196_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0059_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0018_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0815_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0157_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0007_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0066_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1482_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0038_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0071_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0071_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0880_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1456_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0223_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0060_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0886_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0886_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0279_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0279_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1488_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0869_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0869_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1488_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1435_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1435_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0023_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0870_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0003_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0042_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0055_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0055_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0819_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0294_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0294_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0001_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0159_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0159_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0296_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0029_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0029_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1467_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1467_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0130_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0038_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1437_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0038_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1449_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0018_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1408_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0018_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0136_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0287_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0092_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0294_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0294_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0893_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0819_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0055_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0144_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0042_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0144_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0089_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0114_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0869_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0196_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0882_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0073_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0136_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0287_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0136_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0287_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0071_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0092_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0880_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0880_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0059_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0007_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0157_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0007_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0066_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0038_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0038_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1437_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0068_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0068_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0223_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0223_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0060_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0280_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0184_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0280_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0037_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0805_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1407_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0056_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0082_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0082_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1479_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0842_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0803_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0058_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1429_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0078_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0078_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0224_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0224_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0829_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1481_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0065_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0284_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0033_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0261_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0261_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0191_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0838_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1453_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0104_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0104_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1477_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0078_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0058_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0047_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0082_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0069_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1479_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0008_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0056_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0280_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1466_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0199_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1473_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0199_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0054_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0024_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0284_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0284_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0206_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0809_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0089_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0089_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0144_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0893_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0062_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0073_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0073_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0869_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1488_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1480_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0114_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0114_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0038_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1482_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0038_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0059_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0007_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1408_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0007_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0815_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1408_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1495_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0092_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0880_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0001_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0001_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0057_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0296_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0223_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0223_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0060_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0068_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0886_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0068_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0130_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0130_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0040_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_143_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1488_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1435_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1435_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0279_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0279_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0294_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0268_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0294_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0268_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0003_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0144_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0240_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0144_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0089_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0023_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0870_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0040_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0130_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0130_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0296_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0296_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0029_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0159_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0287_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0136_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0287_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0157_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1437_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0058_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0047_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0047_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1477_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0855_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0037_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0041_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0008_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0008_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0037_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0261_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1473_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0199_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0035_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0199_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0866_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0809_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0206_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0206_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0024_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0284_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0033_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0061_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0082_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1479_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1407_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0853_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0805_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0805_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0184_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0037_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0288_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0288_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0019_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_148_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0019_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1429_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1416_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0803_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0033_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0206_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0801_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0829_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1453_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0104_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1412_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0104_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0838_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0199_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0191_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0191_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0810_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0067_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1477_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0047_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0058_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0286_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0008_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0008_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0056_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0037_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0037_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0193_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1485_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0280_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0280_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0288_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0288_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1412_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0104_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0104_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0054_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0035_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0199_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1473_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0035_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1473_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0199_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0206_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0801_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1462_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0024_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0284_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0284_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1481_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1481_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0805_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0184_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0184_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0288_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0288_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0037_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0082_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0061_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0037_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0082_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0193_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0193_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0853_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1407_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1416_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0842_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1416_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_148_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0019_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1483_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0224_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0224_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0067_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0004_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0829_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0065_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0065_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0024_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0801_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0801_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0063_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0191_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0191_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1453_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0268_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1486_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0893_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0893_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0023_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0003_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0144_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0144_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0196_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0114_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0114_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0882_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0882_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0073_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1456_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0071_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0092_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0066_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1482_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0038_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1482_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0059_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0157_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0815_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0007_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0018_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0815_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0886_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0068_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0886_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0001_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0001_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0159_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1484_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0073_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0882_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0073_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0114_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0003_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0042_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0042_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0023_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0268_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0296_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0296_0004_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0029_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0057_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0159_0003_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0159_0002_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1467_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1467_0007_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0059_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1449_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/out_of_focus0059_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1408_0005_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1482_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/DIV2K_0863_0006_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1437_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0287_0001_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1495_0009_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/motion0287_0000_LRx4.png', 'label': 1},\n",
       " {'filename': 'blurred/EBD_1495_0008_LRx4.png', 'label': 1},\n",
       " {'filename': 'clear/EBD_297.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1405.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000079.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0054.png', 'label': 0},\n",
       " {'filename': 'clear/00057_02_00007.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0185.png', 'label': 0},\n",
       " {'filename': 'clear/00118_06_00006.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-cottonbro-6221575.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000333.png', 'label': 0},\n",
       " {'filename': 'clear/00113_01_00003.png', 'label': 0},\n",
       " {'filename': 'clear/light-road-tunnel-subway-line-color-860112-pxhere.com.jpg',\n",
       "  'label': 0},\n",
       " {'filename': 'clear/Flick2K_001789.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1363.png', 'label': 0},\n",
       " {'filename': 'clear/00064_01_00003.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0379.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000124.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_321.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0069.png', 'label': 0},\n",
       " {'filename': 'clear/00002_02_00006.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0055.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001428.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000939.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0755.png', 'label': 0},\n",
       " {'filename': 'clear/00061_02_00004.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0281.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1160.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1559.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0453.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0321.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0490.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-ali-ramazan-ciftci-82252581-14446260.jpg',\n",
       "  'label': 0},\n",
       " {'filename': 'clear/DIV8K_1406.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1360.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_280.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0081.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0057.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001562.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002043.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0446.png', 'label': 0},\n",
       " {'filename': 'clear/motion0183.jpg', 'label': 0},\n",
       " {'filename': 'clear/00046_07_00002.png', 'label': 0},\n",
       " {'filename': 'clear/motion0154.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0296.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001990.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002255.png', 'label': 0},\n",
       " {'filename': 'clear/00071_07_00006.png', 'label': 0},\n",
       " {'filename': 'clear/00086_03_00002.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-andres-ayrton-6551455.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000457.png', 'label': 0},\n",
       " {'filename': 'clear/00010_04_00003.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0240.png', 'label': 0},\n",
       " {'filename': 'clear/00070_01_00006.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0839.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-helen1-21517205.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-hebertsantos-6273274.jpg', 'label': 0},\n",
       " {'filename': 'clear/00112_01_00007.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_88.png', 'label': 0},\n",
       " {'filename': 'clear/00094_03_00003.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0527.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0251.png', 'label': 0},\n",
       " {'filename': 'clear/00115_01_00003.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0020.png', 'label': 0},\n",
       " {'filename': 'clear/00044_09_00003.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0626.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0140.png', 'label': 0},\n",
       " {'filename': 'clear/00088_01_00002.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_454.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000123.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0801.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000679.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002287.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000490.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1370.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1358.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002536.png', 'label': 0},\n",
       " {'filename': 'clear/motion0151.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002523.png', 'label': 0},\n",
       " {'filename': 'clear/motion0187.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0251.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0047.png', 'label': 0},\n",
       " {'filename': 'clear/00057_02_00000.png', 'label': 0},\n",
       " {'filename': 'clear/motion0226.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_247.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-magda-ehlers-pexels-1319572.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_290.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_723.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0443.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1577.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0278.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0235.png', 'label': 0},\n",
       " {'filename': 'clear/motion0224.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002455.png', 'label': 0},\n",
       " {'filename': 'clear/00000_01_00005.png', 'label': 0},\n",
       " {'filename': 'clear/motion0032.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_292.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1011.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0694.png', 'label': 0},\n",
       " {'filename': 'clear/00061_01_00008.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_279.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0865.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1373.png', 'label': 0},\n",
       " {'filename': 'clear/motion0185.jpg', 'label': 0},\n",
       " {'filename': 'clear/00110_02_00007.png', 'label': 0},\n",
       " {'filename': 'clear/00113_01_00007.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001028.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002252.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-cottonbro-6635917.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-cottonbro-7265998.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_1372.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0354.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001571.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0803.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000135.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000121.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-karoldach-409701.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0695.png', 'label': 0},\n",
       " {'filename': 'clear/motion0225.jpg', 'label': 0},\n",
       " {'filename': 'clear/00090_00_00005.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1004.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1373.png', 'label': 0},\n",
       " {'filename': 'clear/motion0033.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002497.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1574.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0546.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0253.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002341.png', 'label': 0},\n",
       " {'filename': 'clear/00015_00_00001.png', 'label': 0},\n",
       " {'filename': 'clear/motion0295.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000967.png', 'label': 0},\n",
       " {'filename': 'clear/motion0256.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001462.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0817.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_800.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-thisisengineering-3912510.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0340.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0469.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1467.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_182.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1315.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_196.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000378.png', 'label': 0},\n",
       " {'filename': 'clear/motion0120.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001700.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002553.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0591.png', 'label': 0},\n",
       " {'filename': 'clear/motion0121.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000392.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001098.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_430.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-alexander-mass-748453803-19583623.jpg',\n",
       "  'label': 0},\n",
       " {'filename': 'clear/DIV2K_0130.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000812.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000027.png', 'label': 0},\n",
       " {'filename': 'clear/motion0243.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0181.png', 'label': 0},\n",
       " {'filename': 'clear/00064_00_00003.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0630.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-houssam-benamara-542776584-16620395.jpg',\n",
       "  'label': 0},\n",
       " {'filename': 'clear/Flick2K_002426.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-tu-nguyen-477344610-15818869.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0383.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001846.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-ekrulila-18777928.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0424.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0250.png', 'label': 0},\n",
       " {'filename': 'clear/motion0057.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000555.png', 'label': 0},\n",
       " {'filename': 'clear/motion0282.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0753.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0626.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002197.png', 'label': 0},\n",
       " {'filename': 'clear/00008_01_00008.png', 'label': 0},\n",
       " {'filename': 'clear/motion0241.jpg', 'label': 0},\n",
       " {'filename': 'clear/00053_00_00004.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-efren-ftz-365656346-14789357.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000179.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-jana-rustmeier-1891721-3508975.jpg', 'label': 0},\n",
       " {'filename': 'clear/00071_00_00000.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001071.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0480.png', 'label': 0},\n",
       " {'filename': 'clear/motion0122.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_619.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1303.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0331.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_802.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-spencer-selover-142259-428321.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0626.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-caio-59596.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001138.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001886.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-yulia-polyakova-73722901-9884586.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0236.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0357.png', 'label': 0},\n",
       " {'filename': 'clear/motion0081.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001699.png', 'label': 0},\n",
       " {'filename': 'clear/00077_00_00002.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1501.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002384.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0353.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-rdne-7755661.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001869.png', 'label': 0},\n",
       " {'filename': 'clear/motion0287.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000785.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-klaus-nielsen-6287300.jpg', 'label': 0},\n",
       " {'filename': 'clear/00087_09_00005.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000008.png', 'label': 0},\n",
       " {'filename': 'clear/00113_08_00008.png', 'label': 0},\n",
       " {'filename': 'clear/00006_06_00006.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0309.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-cup-of-couple-6634474.jpg', 'label': 0},\n",
       " {'filename': 'clear/00013_00_00000.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-anete-lusina-7256738.jpg', 'label': 0},\n",
       " {'filename': 'clear/motion0132.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001060.png', 'label': 0},\n",
       " {'filename': 'clear/motion0126.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002568.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0597.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0390.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-sevenstormphotography-381739.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0568.png', 'label': 0},\n",
       " {'filename': 'clear/motion0245.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-blue-bird-7210698.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000784.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1272.png', 'label': 0},\n",
       " {'filename': 'clear/00077_00_00003.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0240.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-jessef11-1213294.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0580.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002191.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0040.png', 'label': 0},\n",
       " {'filename': 'clear/00031_00_00007.png', 'label': 0},\n",
       " {'filename': 'clear/motion0290.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000779.png', 'label': 0},\n",
       " {'filename': 'clear/00107_00_00001.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0609.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-ekaterina-121008470-9961407.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-karoldach-1013516.jpg', 'label': 0},\n",
       " {'filename': 'clear/00012_06_00003.png', 'label': 0},\n",
       " {'filename': 'clear/00124_06_00005.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-anete-lusina-5240734.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_1488.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0185.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-alexander-mass-748453803-19583626.jpg',\n",
       "  'label': 0},\n",
       " {'filename': 'clear/Flick2K_001248.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001314.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_227.png', 'label': 0},\n",
       " {'filename': 'clear/motion0285.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0740.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0147.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0733.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-shvets-production-9774339.jpg', 'label': 0},\n",
       " {'filename': 'clear/00071_05_00008.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1503.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002386.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0215.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1268.png', 'label': 0},\n",
       " {'filename': 'clear/00052_05_00006.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0412.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_982.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_772.png', 'label': 0},\n",
       " {'filename': 'clear/motion0049.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-nida-kurt-5079840-7810405.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001457.png', 'label': 0},\n",
       " {'filename': 'clear/00056_06_00006.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000775.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_216.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000007.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_564.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0886.png', 'label': 0},\n",
       " {'filename': 'clear/00033_00_00004.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0200.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_160.png', 'label': 0},\n",
       " {'filename': 'clear/00057_06_00000.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1484.png', 'label': 0},\n",
       " {'filename': 'clear/motion0100.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-andiravsanjani-1915182.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_1335.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001278.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0688.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0893.png', 'label': 0},\n",
       " {'filename': 'clear/00053_05_00000.png', 'label': 0},\n",
       " {'filename': 'clear/00055_08_00004.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_571.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000984.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000748.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001456.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001330.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1452.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-david-geib-1265112-3220861.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000210.png', 'label': 0},\n",
       " {'filename': 'clear/motion0060.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001859.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002388.png', 'label': 0},\n",
       " {'filename': 'clear/00072_06_00006.png', 'label': 0},\n",
       " {'filename': 'clear/carl-joseph-jASSGNBObNY-unsplash.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_995.png', 'label': 0},\n",
       " {'filename': 'clear/00016_00_00003.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_229.png', 'label': 0},\n",
       " {'filename': 'clear/motion0248.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0029.png', 'label': 0},\n",
       " {'filename': 'clear/00007_03_00002.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0821.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1323.png', 'label': 0},\n",
       " {'filename': 'clear/motion0116.jpg', 'label': 0},\n",
       " {'filename': 'clear/motion0103.jpg', 'label': 0},\n",
       " {'filename': 'clear/00099_00_00006.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0853.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0149.png', 'label': 0},\n",
       " {'filename': 'clear/00053_05_00003.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_228.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0066.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0015.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-bertellifotografia-573302.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002404.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-nurseryart-346885.jpg', 'label': 0},\n",
       " {'filename': 'clear/motion0077.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_980.png', 'label': 0},\n",
       " {'filename': 'clear/00026_00_00007.png', 'label': 0},\n",
       " {'filename': 'clear/00104_00_00007.png', 'label': 0},\n",
       " {'filename': 'clear/denys-argyriou-FvfK16cHRrE-unsplash.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0217.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0258.png', 'label': 0},\n",
       " {'filename': 'clear/00123_00_00008.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-katlovessteve-568022.jpg', 'label': 0},\n",
       " {'filename': 'clear/00083_09_00004.png', 'label': 0},\n",
       " {'filename': 'clear/00057_00_00000.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000767.png', 'label': 0},\n",
       " {'filename': 'clear/00040_03_00006.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-blue-bird-7242883.jpg', 'label': 0},\n",
       " {'filename': 'clear/motion0265.jpg', 'label': 0},\n",
       " {'filename': 'clear/00118_02_00006.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001096.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000411.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002560.png', 'label': 0},\n",
       " {'filename': 'clear/motion0107.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1093.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0575.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001083.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1291.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001450.png', 'label': 0},\n",
       " {'filename': 'clear/motion0264.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000014.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0165.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1284.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000564.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0415.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1247.png', 'label': 0},\n",
       " {'filename': 'clear/motion0099.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_1521.png', 'label': 0},\n",
       " {'filename': 'clear/00087_07_00004.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1523.png', 'label': 0},\n",
       " {'filename': 'clear/StockCake-Spinning Grinding Wheel_1720264792.jpg',\n",
       "  'label': 0},\n",
       " {'filename': 'clear/motion0070.jpg', 'label': 0},\n",
       " {'filename': 'clear/00067_00_00006.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-ganinph-7760793.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-bohlemedia-1089930.jpg', 'label': 0},\n",
       " {'filename': 'clear/motion0104.png', 'label': 0},\n",
       " {'filename': 'clear/00081_09_00008.png', 'label': 0},\n",
       " {'filename': 'clear/00116_00_00001.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_818.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1319.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0416.png', 'label': 0},\n",
       " {'filename': 'clear/00088_05_00007.png', 'label': 0},\n",
       " {'filename': 'clear/motion0111.jpg', 'label': 0},\n",
       " {'filename': 'clear/00061_00_00000.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1481.png', 'label': 0},\n",
       " {'filename': 'clear/00113_06_00008.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0210.png', 'label': 0},\n",
       " {'filename': 'clear/00089_03_00006.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1456.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_400.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000605.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0006.png', 'label': 0},\n",
       " {'filename': 'clear/00037_00_00008.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_399.png', 'label': 0},\n",
       " {'filename': 'clear/madison-oren-8iN-YTPHAEw-unsplash.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_1126.png', 'label': 0},\n",
       " {'filename': 'clear/00064_08_00003.png', 'label': 0},\n",
       " {'filename': 'clear/motion0267.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001447.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002600.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000771.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1046.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0358.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1536.png', 'label': 0},\n",
       " {'filename': 'clear/sam-pearce-warrilow-hA20BcBxMUQ-unsplash.jpg',\n",
       "  'label': 0},\n",
       " {'filename': 'clear/pexels-khoa-vo-2347168-4958618.jpg', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001805.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0074.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0129.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001408.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000702.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0128.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0615.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0359.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1020.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-cottonbro-7243952.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0539.png', 'label': 0},\n",
       " {'filename': 'clear/00000_04_00006.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_671.png', 'label': 0},\n",
       " {'filename': 'clear/motion0163.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0262.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0465.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001541.png', 'label': 0},\n",
       " {'filename': 'clear/00110_01_00002.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000703.png', 'label': 0},\n",
       " {'filename': 'clear/00007_01_00008.png', 'label': 0},\n",
       " {'filename': 'clear/motion0215.jpg', 'label': 0},\n",
       " {'filename': 'clear/00112_02_00005.png', 'label': 0},\n",
       " {'filename': 'clear/00072_01_00003.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1593.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002464.png', 'label': 0},\n",
       " {'filename': 'clear/00006_09_00003.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1550.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1578.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_1169.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0300.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-german-pineda-239179269-12251315.jpg', 'label': 0},\n",
       " {'filename': 'clear/00093_00_00003.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0116.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0922.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-ketut-subiyanto-4350288.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0665.png', 'label': 0},\n",
       " {'filename': 'clear/00046_04_00003.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_458.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000107.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0400.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002260.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1397.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000477.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000304.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0499.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001998.png', 'label': 0},\n",
       " {'filename': 'clear/motion0160.jpg', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0507.png', 'label': 0},\n",
       " {'filename': 'clear/00110_01_00001.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_263.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-olly-3761509.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-thatguycraig000-1546901.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_1590.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002467.png', 'label': 0},\n",
       " {'filename': 'clear/00093_00_00002.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1584.png', 'label': 0},\n",
       " {'filename': 'clear/00007_02_00003.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001191.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002305.png', 'label': 0},\n",
       " {'filename': 'clear/00070_04_00004.png', 'label': 0},\n",
       " {'filename': 'clear/damien-ramage-Ixi73IUcmfE-unsplash.jpg', 'label': 0},\n",
       " {'filename': 'clear/pexels-pixabay-247195.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0015.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-lucasallmann-612891.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0700.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-vlada-karpovich-4050340.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0927.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001591.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000857.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1437.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002271.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000301.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002516.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-nandhukumar-312839.jpg', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0410.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001553.png', 'label': 0},\n",
       " {'filename': 'clear/motion0207.jpg', 'label': 0},\n",
       " {'filename': 'clear/00094_00_00007.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002112.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0926.png', 'label': 0},\n",
       " {'filename': 'clear/motion0005.jpg', 'label': 0},\n",
       " {'filename': 'clear/00058_03_00002.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000275.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_1595.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001194.png', 'label': 0},\n",
       " {'filename': 'clear/00070_09_00006.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0310.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0529.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0448.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000288.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0501.png', 'label': 0},\n",
       " {'filename': 'clear/motion0239.jpg', 'label': 0},\n",
       " {'filename': 'clear/motion0205.jpg', 'label': 0},\n",
       " {'filename': 'clear/motion0198.jpg', 'label': 0},\n",
       " {'filename': 'clear/EBD_1352.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0348.png', 'label': 0},\n",
       " {'filename': 'clear/pexels-reyna-3007759.jpg', 'label': 0},\n",
       " {'filename': 'clear/00056_07_00003.png', 'label': 0},\n",
       " {'filename': 'clear/00035_01_00007.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_311.png', 'label': 0},\n",
       " {'filename': 'clear/motion0199.png', 'label': 0},\n",
       " {'filename': 'clear/EBD_305.png', 'label': 0},\n",
       " {'filename': 'clear/out_of_focus0065.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_001587.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0639.png', 'label': 0},\n",
       " {'filename': 'clear/DIV2K_0163.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_002139.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0064.png', 'label': 0},\n",
       " {'filename': 'clear/00106_02_00006.png', 'label': 0},\n",
       " {'filename': 'clear/Flick2K_000262.png', 'label': 0},\n",
       " {'filename': 'clear/00072_01_00006.png', 'label': 0},\n",
       " {'filename': 'clear/DIV8K_0500.png', 'label': 0},\n",
       " {'filename': 'clear/00064_09_00003.png', 'label': 0}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1000\n",
      "Blurred: 500 Clear: 500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/subset_dataset/metadata.csv\")\n",
    "\n",
    "print(\"Total images:\", len(df))\n",
    "print(\"Blurred:\", len(df[df[\"label\"] == 1]), \"Clear:\", len(df[df[\"label\"] == 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\"label\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split temp into validation (50% of temp = 15% of total) and test (50% of temp = 15% of total)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/subset_dataset/train_metadata.csv\", index=False)\n",
    "val_df.to_csv(\"/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/subset_dataset/val_metadata.csv\", index=False)\n",
    "test_df.to_csv(\"/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/subset_dataset/test_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 700 images\n",
      "Validation set: 150 images\n",
      "Test set: 150 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\", len(train_df), \"images\")\n",
    "print(\"Validation set:\", len(val_df), \"images\")\n",
    "print(\"Test set:\", len(test_df), \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init for loading the csv, dir, and transform \n",
    "len for the nubmer of the smaples \n",
    "getitem for getting the image and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BlurDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):# constructor \n",
    "        path = os.path.join(root_dir,csv_file)\n",
    "        self.metadata = pd.read_csv(path)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx): # image and its label \n",
    "        img_name = os.path.join(self.root_dir, self.metadata.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = int(self.metadata.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize all images to 224x224 (this is the standard size for ResNet18).\n",
    "\n",
    "Convert images to PyTorch tensors.\n",
    "\n",
    "Normalize the images to [-1, 1] range (because mean = 0.5 and std = 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),      \n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle=True for training, which is important for better learning.\n",
    "\n",
    "shuffle=False for validation and testing (normal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader in PyTorch is a tool that helps you:\n",
    "- Automatically load the data in small batches (not one by one).\n",
    "- Shuffle the data (mix it randomly for training).\n",
    "- Feed batches to the model during training, validation, and testing.\n",
    "- Make loading faster (parallel loading if needed — not very important on M2, but still good)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "root_dir=\"/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/subset_dataset\"\n",
    "train_dataset = BlurDataset(csv_file='train_metadata.csv', root_dir=root_dir, transform=transform)\n",
    "val_dataset = BlurDataset(csv_file='val_metadata.csv', root_dir=root_dir, transform=transform)\n",
    "test_dataset = BlurDataset(csv_file='test_metadata.csv', root_dir=root_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=32, shuffle=False)\n",
    "# In validation and testing, you dont shuffle because you want to measure real performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "took a ResNet18 that is already pretrained on ImageNet \n",
    "The orginal ResNet has 1000 ouputs (for 1000 ImageNet classes )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class BlurDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlurDetectionModel, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Replace the final fc (fully connected layer) to output 2  (blurred/not blurred)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7*7*64*3 + 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/computervision/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/computervision/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = resnet18(pretrained=True)\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# flexing my device capabilites \n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/computervision/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/computervision/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/masaaladwan/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:11<00:00, 4.13MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = BlurDetectionModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Train Loss: 0.5119 | Train Acc: 84.71% | Val Loss: 2.7018 | Val Acc: 48.00%\n",
      "Epoch [2/10] Train Loss: 0.1001 | Train Acc: 96.71% | Val Loss: 0.7112 | Val Acc: 83.33%\n",
      "Epoch [3/10] Train Loss: 0.1169 | Train Acc: 95.29% | Val Loss: 0.4215 | Val Acc: 88.00%\n",
      "Epoch [4/10] Train Loss: 0.0816 | Train Acc: 96.57% | Val Loss: 0.1407 | Val Acc: 94.00%\n",
      "Epoch [5/10] Train Loss: 0.1138 | Train Acc: 95.86% | Val Loss: 0.1442 | Val Acc: 93.33%\n",
      "Epoch [6/10] Train Loss: 0.0853 | Train Acc: 97.29% | Val Loss: 0.2544 | Val Acc: 89.33%\n",
      "Epoch [7/10] Train Loss: 0.0674 | Train Acc: 97.57% | Val Loss: 0.1372 | Val Acc: 94.67%\n",
      "Epoch [8/10] Train Loss: 0.0405 | Train Acc: 98.71% | Val Loss: 0.8317 | Val Acc: 82.00%\n",
      "Epoch [9/10] Train Loss: 0.0602 | Train Acc: 97.86% | Val Loss: 0.2677 | Val Acc: 93.33%\n",
      "Epoch [10/10] Train Loss: 0.0347 | Train Acc: 98.71% | Val Loss: 0.1017 | Val Acc: 94.67%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training and Validation\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # clear the previous gradients\n",
    "        outputs = model(images) # forward pass \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward() # backprogoagtion \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100. * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "          f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% '\n",
    "          f'| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clear       0.99      0.95      0.97        75\n",
      "     Blurred       0.95      0.99      0.97        75\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=['Clear', 'Blurred'])\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Clear', 'Blurred'], yticklabels=['Clear', 'Blurred'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'blur_detection_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipped \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR Images: 2393\n",
      "Masks: 2718\n",
      "LR Defocus: 800\n",
      "LR Motion: 400\n"
     ]
    }
   ],
   "source": [
    "# i will start with reducing the dataset size \n",
    "data_dir = '/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR'\n",
    "\n",
    "image_paths = sorted(glob.glob(f'{data_dir}/ALL_HR/*.png'))\n",
    "mask_paths = sorted(glob.glob(f'{data_dir}/ALL_mask/*.png'))\n",
    "lr_defocus = sorted(glob.glob(f'{data_dir}/valid/defocus/LR/X4/*.png'))\n",
    "lr_motion = sorted(glob.glob(f'{data_dir}/valid/motion/LR/X4/*.png'))\n",
    "\n",
    "print(f'HR Images: {len(image_paths)}')\n",
    "print(f'Masks: {len(mask_paths)}')\n",
    "print(f'LR Defocus: {len(lr_defocus)}')\n",
    "print(f'LR Motion: {len(lr_motion)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these is smth wrong, lets check the formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR Images: 2931\n",
      "HR Formats: {'.png': 2393, '.jpg': 538}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Masks: 2931\n",
      "Mask Formats: {'.png': 2718, '.jpg': 213}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LR Defocus: 800\n",
      "LR Defocus Formats: {'.png': 800}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LR Motion: 400\n",
      "LR Motion Formats: {'.png': 400}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#multiple formats\n",
    "image_paths = sorted(glob.glob(f'{data_dir}/ALL_HR/*.*'))\n",
    "mask_paths = sorted(glob.glob(f'{data_dir}/ALL_mask/*.*'))\n",
    "lr_defocus = sorted(glob.glob(f'{data_dir}/valid/defocus/LR/X4/*.*'))\n",
    "lr_motion = sorted(glob.glob(f'{data_dir}/valid/motion/LR/X4/*.*'))\n",
    "\n",
    "def count_formats(paths):\n",
    "    formats = {}\n",
    "    for p in paths:\n",
    "        # function thats split the path into root and extension \n",
    "        # https://www.geeksforgeeks.org/python-os-path-splitext-method/\n",
    "        ext = os.path.splitext(p)[1].lower()  # e.g., .png, .jpg\n",
    "        formats[ext] = formats.get(ext, 0) + 1\n",
    "    return formats\n",
    "\n",
    "print(f'HR Images: {len(image_paths)}')\n",
    "print(f'HR Formats: {count_formats(image_paths)}')\n",
    "print(\"--\"*50)\n",
    "\n",
    "print(f'Masks: {len(mask_paths)}')\n",
    "print(f'Mask Formats: {count_formats(mask_paths)}')\n",
    "print(\"--\"*50)\n",
    "\n",
    "print(f'LR Defocus: {len(lr_defocus)}')\n",
    "print(f'LR Defocus Formats: {count_formats(lr_defocus)}')\n",
    "print(\"--\"*50)\n",
    "\n",
    "print(f'LR Motion: {len(lr_motion)}')\n",
    "print(f'LR Motion Formats: {count_formats(lr_motion)}')\n",
    "print(\"--\"*50)\n",
    "\n",
    "train_valid_class = np.load('/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/train_validation_class.npy') \n",
    "defocus_motion_class = np.load(f'/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR/defocus_motion_class.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in .npy: 2931\n",
      "Train (0): 2811, Valid (1): 120\n",
      "Defocus (0): 2566, Motion (1): 365\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Total Images in .npy: {len(train_valid_class)}')\n",
    "print(f'Train (0): {np.sum(train_valid_class == 0)}, Valid (1): {np.sum(train_valid_class == 1)}')\n",
    "print(f'Defocus (0): {np.sum(defocus_motion_class == 0)}, Motion (1): {np.sum(defocus_motion_class == 1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the problems :\n",
    "1. the validation is 4% of the whole data \n",
    "2. defocus outnumbers motion, so the subset mist balance these to aviode bias \n",
    "\n",
    "\n",
    " to reduce the dataset : \n",
    " select 1000 HR images and 200 LR, but : \n",
    " - the train/vaild/test will be 80/10/10 \n",
    " - the format is mixed and its okay for now \n",
    " - im not sure how to deal with defocus. motion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset HR images\n",
    "np.random.seed(42)\n",
    "subset_size = 1000\n",
    "\n",
    "defocus_indices = np.where(defocus_motion_class == 0)[0]  # 2566\n",
    "motion_indices = np.where(defocus_motion_class == 1)[0]   # 365\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "defocus_subset = np.random.choice(defocus_indices, size=int(0.80 * subset_size), replace=False) \n",
    "motion_subset = np.random.choice(motion_indices, size=int(0.20 * subset_size), replace=False)  \n",
    "\n",
    "subset_indices = np.concatenate([defocus_subset, motion_subset])\n",
    "\n",
    "np.random.shuffle(subset_indices)\n",
    "\n",
    "print(defocus_subset.size)\n",
    "print(motion_subset.size)\n",
    "print(subset_indices.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_paths_subset = [image_paths[i] for i in subset_indices]\n",
    "\n",
    "train_valid_class_subset = train_valid_class[subset_indices]\n",
    "\n",
    "defocus_motion_class_subset = defocus_motion_class[subset_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Adjust train/valid split (since valid is small)\n",
    "valid_subset_indices = np.where(train_valid_class_subset == 1)[0]  \n",
    "train_subset_indices = np.where(train_valid_class_subset == 0)[0]\n",
    "print(train_subset_indices.size)\n",
    "print(valid_subset_indices.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(valid_subset_indices) < 200:\n",
    "    extra_valid = np.random.choice(train_subset_indices, size=200 - len(valid_subset_indices), replace=False)\n",
    "    train_valid_class_subset[extra_valid] = 1\n",
    "    valid_subset_indices = np.where(train_valid_class_subset == 1)[0]\n",
    "    train_subset_indices = np.where(train_valid_class_subset == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Subset Train (0): 800\n",
      "Adjusted Subset Valid (1): 200\n",
      "Subset HR Images: 1000\n",
      "Subset Defocus (0): 880\n",
      "Subset Motion (1): 120\n"
     ]
    }
   ],
   "source": [
    "print(f'Adjusted Subset Train (0): {len(train_subset_indices)}')\n",
    "print(f'Adjusted Subset Valid (1): {len(valid_subset_indices)}')\n",
    "print(f'Subset HR Images: {len(image_paths_subset)}')\n",
    "print(f'Subset Defocus (0): {np.sum(defocus_motion_class_subset == 0)}')\n",
    "print(f'Subset Motion (1): {np.sum(defocus_motion_class_subset == 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser('/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR')\n",
    "output_dir = os.path.expanduser('/Users/masaaladwan/Desktop/Leading point /ComputerVision/BlurDataset') \n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = sorted(glob.glob(f'{data_dir}/ALL_HR/*.*'))\n",
    "train_valid_class = np.load(f'{data_dir}/train_validation_class.npy')\n",
    "defocus_motion_class = np.load(f'{data_dir}/defocus_motion_class.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "subset_size = 1000\n",
    "defocus_indices = np.where(defocus_motion_class == 0)[0]\n",
    "motion_indices = np.where(defocus_motion_class == 1)[0]\n",
    "defocus_subset = np.random.choice(defocus_indices, size=int(0.80 * subset_size), replace=False)\n",
    "motion_subset = np.random.choice(motion_indices, size=int(0.20 * subset_size), replace=False)\n",
    "subset_indices = np.concatenate([defocus_subset, motion_subset])\n",
    "np.random.shuffle(subset_indices)\n",
    "\n",
    "image_paths_subset = [image_paths[i] for i in subset_indices]\n",
    "train_valid_class_subset = train_valid_class[subset_indices]\n",
    "valid_subset_indices = np.where(train_valid_class_subset == 1)[0]\n",
    "train_subset_indices = np.where(train_valid_class_subset == 0)[0]\n",
    "if len(valid_subset_indices) < 200:\n",
    "    extra_valid = np.random.choice(train_subset_indices, size=200 - len(valid_subset_indices), replace=False)\n",
    "    train_valid_class_subset[extra_valid] = 1\n",
    "defocus_motion_class_subset = defocus_motion_class[subset_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'valid']:\n",
    "    for blur_type in ['defocus', 'motion']:\n",
    "        os.makedirs(f'{split}/{blur_type}/HR', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Linking HR Images: 100%|██████████| 1000/1000 [00:00<00:00, 19816.51it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(image_paths_subset)), desc=\"Linking HR Images\"):\n",
    "    img_name = os.path.basename(image_paths_subset[i])\n",
    "    src_img = os.path.abspath(image_paths_subset[i])\n",
    "    try:\n",
    "        if train_valid_class_subset[i] == 0:  # Train\n",
    "            if defocus_motion_class_subset[i] == 0:  # Defocus\n",
    "                os.symlink(src_img, f'train/defocus/HR/{img_name}')\n",
    "            else:  # Motion\n",
    "                os.symlink(src_img, f'train/motion/HR/{img_name}')\n",
    "        else:  # Validation\n",
    "            if defocus_motion_class_subset[i] == 0:  # Defocus\n",
    "                os.symlink(src_img, f'valid/defocus/HR/{img_name}')\n",
    "            else:  # Motion\n",
    "                os.symlink(src_img, f'valid/motion/HR/{img_name}')\n",
    "    except FileExistsError:\n",
    "        pass  # Skip if symlink exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Defocus HR: 843\n",
      "Train Motion HR: 168\n",
      "Valid Defocus HR: 276\n",
      "Valid Motion HR: 71\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Defocus HR: {len(glob.glob(\"train/defocus/HR/*.*\"))}')\n",
    "print(f'Train Motion HR: {len(glob.glob(\"train/motion/HR/*.*\"))}')\n",
    "print(f'Valid Defocus HR: {len(glob.glob(\"valid/defocus/HR/*.*\"))}')\n",
    "print(f'Valid Motion HR: {len(glob.glob(\"valid/motion/HR/*.*\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data to Train , valid , Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This script splits the ReBlurSR dataset into train, validation, and test sets for a blur detection model (sharp vs. blurred). It adapts the original train_validation_split.py to organize high-resolution (HR, sharp) and low-resolution (LR, blurred) images into a BlurDataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/masaaladwan/Desktop/Leading point /ComputerVision/ReBlurSR'  \n",
    "output_dir = '/Users/masaaladwan/Desktop/Leading point /ComputerVision/BlurDataset'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_class = np.load(f'{data_dir}/train_validation_class.npy')\n",
    "defocus_motion_class = np.load(f'{data_dir}/defocus_motion_class.npy')\n",
    "\n",
    "# Print shapes and unique values to understand the data\n",
    "print(f'Train/Valid Classes: {train_valid_class.shape}, Values: {np.unique(train_valid_class)}')\n",
    "print(f'Defocus/Motion Classes: {defocus_motion_class.shape}, Values: {np.unique(defocus_motion_class)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.bincount(train_valid_class))  # Counts of 0 (train) and 1 (validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of .npy files as label sheets for a box of photos:\n",
    "\n",
    "Each photo (image in ALL_HR) has a sticky note with two labels:\n",
    "One says “train” or “validation” (train_validation_class.npy).\n",
    "Another says “defocus” or “motion” (defocus_motion_class.npy).\n",
    "The script reads these labels to sort photos into folders (train/defocus, valid/motion, etc.).\n",
    "For your blur detection, you’re using these labels to decide which photos are “sharp” (HR) and which are “blurred” (LR or synthetic), then organizing them into BlurDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Collect image paths\n",
    "# Use glob to get sorted lists of HR images and masks\n",
    "image_paths = sorted(glob.glob(f'{data_dir}/ALL_HR/*'))\n",
    "mask_paths = sorted(glob.glob(f'{data_dir}/ALL_mask/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'HR Images: {len(image_paths)}')\n",
    "print(f'Masks: {len(mask_paths)}')\n",
    "assert len(image_paths) == len(mask_paths) == len(train_valid_class), 'Mismatch in file counts!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folders for original train/valid (HR and masks) and BlurDataset for classification\n",
    "os.makedirs('train/defocus/HR', exist_ok=True)\n",
    "os.makedirs('train/defocus/mask', exist_ok=True)\n",
    "os.makedirs('train/motion/HR', exist_ok=True)\n",
    "os.makedirs('train/motion/mask', exist_ok=True)\n",
    "os.makedirs('valid/defocus/HR', exist_ok=True)\n",
    "os.makedirs('valid/defocus/mask', exist_ok=True)\n",
    "os.makedirs('valid/motion/HR', exist_ok=True)\n",
    "os.makedirs('valid/motion/mask', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{output_dir}/train/sharp', exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/train/blurred', exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/val/sharp', exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/val/blurred', exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/test/sharp', exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/test/blurred', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split and copy HR images and masks\n",
    "# Copy HR images and masks to train or valid based on train_valid_class and defocus_motion_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(image_paths)), desc=\"Splitting HR and masks\"):\n",
    "    img_name = os.path.basename(image_paths[i])\n",
    "    mask_name = os.path.basename(mask_paths[i])\n",
    "\n",
    "    if train_valid_class[i] == 0:  # Train\n",
    "        if defocus_motion_class[i] == 0:  # Defocus\n",
    "            shutil.copy(image_paths[i], f'train/defocus/HR/{img_name}')\n",
    "            shutil.copy(mask_paths[i], f'train/defocus/mask/{mask_name}')\n",
    "        else:  # Motion\n",
    "            shutil.copy(image_paths[i], f'train/motion/HR/{img_name}')\n",
    "            shutil.copy(mask_paths[i], f'train/motion/mask/{mask_name}')\n",
    "            \n",
    "    else:  # Validation\n",
    "        if defocus_motion_class[i] == 0:  # Defocus\n",
    "            shutil.copy(image_paths[i], f'valid/defocus/HR/{img_name}')\n",
    "            shutil.copy(mask_paths[i], f'valid/defocus/mask/{mask_name}')\n",
    "        else:  # Motion\n",
    "            shutil.copy(image_paths[i], f'valid/motion/HR/{img_name}')\n",
    "            shutil.copy(mask_paths[i], f'valid/motion/mask/{mask_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whats the tqdm , it python library that adds a progress bat to loops showing how much work is done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create BlurDataset for classification\n",
    "# Organize HR (sharp) and LR (blurred) images into BlurDataset\n",
    "# Split valid into validation and test sets\n",
    "# Use synthetic blur for train/blurred (handled later in PyTorch dataset)\n",
    "\n",
    "train_sharp = list(Path('train/defocus/HR').glob('*.png')) + \\\n",
    "              list(Path('train/motion/HR').glob('*.png'))\n",
    "\n",
    "valid_sharp = list(Path('valid/defocus/HR').glob('*.png')) + \\\n",
    "              list(Path('valid/motion/HR').glob('*.png'))\n",
    "\n",
    "blurred = list(Path(f'{data_dir}/valid/defocus/LR/X4').glob('*.png')) + \\\n",
    "          list(Path(f'{data_dir}/valid/motion/LR/X4').glob('*.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlurDataset/\n",
    "├── train/\n",
    "│   ├── sharp/    # Images from train_sharp\n",
    "│   └── blurred/  # Empty, synthetic blur added later\n",
    "├── val/\n",
    "│   ├── sharp/    # Half of valid_sharp\n",
    "│   └── blurred/  # Half of blurred\n",
    "├── test/\n",
    "│   ├── sharp/    # Half of valid_sharp\n",
    "│   └── blurred/  # Half of blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split valid images into val and test\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(valid_sharp)\n",
    "np.random.shuffle(blurred)\n",
    "val_sharp = valid_sharp[:len(valid_sharp)//2]\n",
    "test_sharp = valid_sharp[len(valid_sharp)//2:]\n",
    "val_blurred = blurred[:len(blurred)//2]\n",
    "test_blurred = blurred[len(blurred)//2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to BlurDataset\n",
    "for img in tqdm(train_sharp, desc=\"Copying train sharp\"):\n",
    "    shutil.copy(img, f'{output_dir}/train/sharp/{img.name}')\n",
    "for img in tqdm(val_sharp, desc=\"Copying val sharp\"):\n",
    "    shutil.copy(img, f'{output_dir}/val/sharp/{img.name}')\n",
    "for img in tqdm(test_sharp, desc=\"Copying test sharp\"):\n",
    "    shutil.copy(img, f'{output_dir}/test/sharp/{img.name}')\n",
    "for img in tqdm(val_blurred, desc=\"Copying val blurred\"):\n",
    "    shutil.copy(img, f'{output_dir}/val/blurred/{img.name}')\n",
    "for img in tqdm(test_blurred, desc=\"Copying test blurred\"):\n",
    "    shutil.copy(img, f'{output_dir}/test/blurred/{img.name}')\n",
    "    \n",
    "print('BlurDataset created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Verify the split\n",
    "# Check the number of images in each folder to ensure the split is balanced\n",
    "def count_images(folder):\n",
    "    return len(list(Path(folder).glob('*.png')))\n",
    "\n",
    "print('Train Sharp:', count_images(f'{output_dir}/train/sharp'))\n",
    "print('Train Blurred:', count_images(f'{output_dir}/train/blurred'))\n",
    "print('Val Sharp:', count_images(f'{output_dir}/val/sharp'))\n",
    "print('Val Blurred:', count_images(f'{output_dir}/val/blurred'))\n",
    "print('Test Sharp:', count_images(f'{output_dir}/test/sharp'))\n",
    "print('Test Blurred:', count_images(f'{output_dir}/test/blurred'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Visualize sample images\n",
    "# Display a sharp and blurred image to confirm the dataset\n",
    "sharp_img = Image.open(next(Path(f'{output_dir}/val/sharp').glob('*.png')))\n",
    "blurred_img = Image.open(next(Path(f'{output_dir}/val/blurred').glob('*.png')))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sharp_img)\n",
    "plt.title('Sharp')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(blurred_img)\n",
    "plt.title('Blurred')\n",
    "plt.axis('off')\n",
    "plt.savefig(f'{output_dir}/sample_images.png')  # Save plot for reference\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 9: Next steps\n",
    "# - Use BlurDataset in a PyTorch dataset class (see previous pipeline)\n",
    "# - Apply synthetic blur to train/sharp images for the blurred class\n",
    "# - Train a ResNet18 model for blur detection\n",
    "# - Save results and script to /content/drive/MyDrive/BlurDetection\n",
    "print('Script completed! Next: Use BlurDataset in PyTorch for blur detection.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/computervision/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/computervision/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlurDetectionModel(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BlurDetectionModel()  # Create model instance\n",
    "model.load_state_dict(torch.load('blur_detection_model.pth', map_location=device))  \n",
    "model.to(device)\n",
    "model.eval()  # Important! Set it to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Clear\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# Define the same transforms you used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "img_path = '/Users/masaaladwan/Downloads/IMG_8276 2.png'\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "image = transform(image)  # Apply the same transforms\n",
    "image = image.unsqueeze(0)  # Add batch dimension (1, 3, 224, 224)\n",
    "\n",
    "# Move image to device\n",
    "image = image.to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "# Map output to class\n",
    "classes = ['Clear', 'Blurred']\n",
    "print(f\"Prediction: {classes[predicted.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
